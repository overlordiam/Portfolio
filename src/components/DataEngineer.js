// src/components/WebDeveloper.js
import React from "react";
import SectionTemplate from "./SectionTemplate";

const DataEngineer = () => {
  const projects = [
    { name: "Azure Data-Engineering Pipeline", description: "This project illustrates the implementation of a comprehensive, end-to-end data pipeline using Azure's cloud ecosystem and Delta Lake technology. It covers the entire data lifecycle, from ingestion to analysis, by integrating various Azure services and tools to ensure efficient and reliable data processing.", storyTime: "I have performed many machine learning tasks and often wondered how the data arrived in such a neat and compact form.  I created this project to understand the transformation from chaos to order. This was my first data engineering project. Taking my first step in this field was a tough one. However, while executing this project, things fell in place one by one.", technologies: "Python, PySpark, SQL, Databricks, Azure Suite, PowerBI", link: "https://github.com/overlordiam/Azure-DE-F1" },
    { name: "Real-Time Wikimedia Data Pipeline", description: "This project demonstrates a comprehensive, end-to-end data pipeline that captures, processes, and analyzes real-time data from Wikimedia. It consists of producers reading data from Wikimedia, redirected by the Kafka brokers to subscribed consumers and finally resolved into an ElasticSearch database.", storyTime: "I made this project to solely learn Kafka. As a web developer as well, I understood how imperative Kafka is to the scalability of applications. This is a mini-project made to get my hands dirty with the fundamentals of Kafka. Stay tuned to learn about the new project!!", technologies: "Java, Kafka, Zookeeper, ElasticSearch", link: "https://github.com/overlordiam/kafka-project" },
    { name: "Real-Time Stock Market Data Pipeline: From Streaming to Analytics", description: "This end-to-end project demonstrates the implementation of a robust, real-time stock market data pipeline leveraging Python, Apache Kafka, and various AWS services. The pipeline efficiently handles streaming stock data, processes it, and makes it available for ad-hoc analysis.", storyTime: "I came across this article stating that AWS offers free credits to access their products. I have used the S3 bucket before for one of my internships, but hadn't really delved into the other services. This project was a very good starter into the data engineering services of AWS.", technologies: "Python, Kafka, SQL, AWS Glue, Athena, EC2, S3", link: "https://github.com/overlordiam/Kafka-AWS-project" },
    { name: "Automated Data Pipeline with Airflow", description: "This project aims to develop a robust and scalable data pipeline that automates the process of extracting data from Google's API, transforming it and storing it in an Amazon S3 bucket.", storyTime: "My motivation to learn Airflow stems from the countless resources pointing to its significance in data orchestration.  Having previously learned Azure Data Factory, I initially assumed the two would be quite similar. However, I quickly realized how mistaken I was. This project has truly taught me the value of programmatically creating orchestrations, in contrast to the drag-and-drop interface I was accustomed to.", technologies: "Python, Pandas, Apache Airflow, AWS S3", link: "https://github.com/overlordiam/Automated-Data-Pipeline-Airflow" },
  ];

  return <SectionTemplate title="The Data Engineeting Maestro (On the way to becoming one!!)" projects={projects} />;
};

export default DataEngineer;
